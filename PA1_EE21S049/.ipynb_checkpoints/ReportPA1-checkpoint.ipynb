{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**EE5179:Deep Learning for Imaging**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Programming Assignment 1: MNIST Digit Classification using MLP</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Dataloading](#paragraph1)\n",
    "3. [One hot encoding](#subparagraph1)\n",
    "4. [Forward Propogation](#paragraph2)\n",
    "5. [Backward Propogation](#paragraph3)\n",
    "6. [Cost Function](#cost)\n",
    "7. [Parameter Updation](#parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$I/P$ → $h_1$ (500) → $h_2$ (250) → $h_3$ (100) → $O/P$ \\\n",
    "Input layer is (784 , Number of classes) \\\n",
    "Hidden Layer 1 (500) \\\n",
    "Hiden Layer 2  (250) \\\n",
    "Hidden Layer 3 (100) \\\n",
    "Output Layer (10,Number of classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Sigmoid** \n",
    "*sigmoid($x$)* = $\\frac{1}{1+exp(-x)} $\n",
    "\n",
    "2. **Tanh** \n",
    "*f($x$)* = $\\frac{exp(x)+exp(-x)}{exp(x)-exp(-x)}$\n",
    "\n",
    "3. **ReLu** \n",
    "*f($x$)* = *max*(0,$x$)\n",
    "\n",
    "4. **Softmax**\n",
    "\n",
    "*softmax($z_i$)* = $\\frac{exp(z_i)}{\\sum exp(z_j)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common initialization scheme for deep NNs is called Glorot (also known as Xavier) Initialization. The idea is to initialize each weight with a small Gaussian value with mean = 0.0 and variance based on the fan-in and fan-out of the weight.\n",
    "\n",
    "For example, each weight that connects an input node to a hidden node has fan-in of the number of input nodes and fan-out of the number of hidden nodes. In pseudo-code the initialization is: \\\n",
    "$w_ij$ ∼ $U$[−$M$, $M$ ] \\\n",
    "$M$ = $\\sqrt\\frac{6}{(N_i+N_o)}$\n",
    "where,$N_i$ is the number of inputs to the weight tensor and $N_o$ is the number of outputs in the weight tensor\n",
    "```\n",
    "def initialise_parameter(dim):\n",
    "  np.random.seed(8)\n",
    "  parameters = {}\n",
    "  L = len(dim)\n",
    "  for i in range(1, L):\n",
    "    Ni = dim[i-1]\n",
    "    No = dim[i]\n",
    "    M = np.sqrt(6/(Ni+No))\n",
    "    p = np.random.uniform(-M, M, No*Ni)\n",
    "      \n",
    "```\n",
    "\n",
    "All the biases in the model are initialized with zeros.\n",
    "```np.zeros((dim[i], 1))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the Weights and the Biases \\\n",
    "```         \n",
    "            W1 (500, 784)\n",
    "            b1 (500, 1)\n",
    "            W2 (250, 500)\n",
    "            b2 (250, 1)\n",
    "            W3 (100, 250)\n",
    "            b3 (100, 1)\n",
    "            W4 (10, 100) \n",
    "            b4 (10, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation (or forward pass) refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer. We now work step-by-step through the mechanics of a neural network with three hidden layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### <center> $Z_1$ = $W_1$ \\* $X$ + $b_1$       &nbsp; &nbsp; &nbsp; &nbsp;         Input to $h_1$ </center>  \n",
    "#####        <center>  $Z_1$ = (500,784) *(784,60000) + (500,1) = (500,60000)</center>\n",
    "##### <center> $A_1$ = *sigmoid*($Z_1$)  </center>  \n",
    "#####        <center> (500,60000)</center>\n",
    "##### <center> $Z_2$ = $W_2$ \\* $A_1$ + $b_2$                    </center>  \n",
    "#####        <center>  $Z_2$ = (250,500) *(500,60000) + (250,1) = (250,60000)</center>\n",
    "##### <center> $A_2$ = *sigmoid*($Z_2$)   </center>  \n",
    "#####        <center> (2500,60000)</center>\n",
    "##### <center> $Z_3$ = $W_3$ \\* $A_2$ + $b_3$                  </center>  \n",
    "#####        <center>  $Z_3$ = (100,250) *(250,60000) + (100,1) = (100,60000)</center>\n",
    "##### <center> $A_3$ = *sigmoid*($Z_3$)   </center>  \n",
    "#####        <center> (100,60000)</center>\n",
    "##### <center> $Z_4$ = $W_4$ \\* $A_3$ + $b_4$               </center>  \n",
    "#####        <center>  $Z_4$ = (10,100) *(100,60000) + (10,1) = (10,60000)</center>\n",
    "##### <center> $A_4$ =  *softmax* ($Z_4$)   </center>  \n",
    "#####        <center> (10,60000)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propogation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and Confusion Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "* Sigmoid \n",
    "* Tanh\n",
    "* ReLu \\\n",
    " All plots and numbers to be included in a table \n",
    "\n",
    " | Activation Function     | Train Accuracy|Test Accuracy|\n",
    "| ----------- | ----------- |--------|\n",
    "| Sigmoid    |     |\n",
    "| Tanh   |        |\n",
    "| Relu |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package \n",
    "Pytorch\n",
    "Plots and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (308070759.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    \\documentclass[12pt,a4paper]{article}\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "\\documentclass[12pt,a4paper]{article}\n",
    "\n",
    "\\usepackage{amsmath}\n",
    "\\usepackage[section]{algorithm}\n",
    "\\usepackage[numbered]{algo}\n",
    "\\usepackage{enumerate}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\begin{algorithm}[h]\n",
    "  \\small\n",
    "  \\begin{enumerate}[(1)]\n",
    "    \\item For a sample $(x_n ,y^*_n)$, propagate the input $x_n$ through the\n",
    "    network to compute the outputs $(v_{i_1}, \\ldots, v_{i_{|V|}})$ (in topological order).\n",
    "    \\vspace{-6px}\n",
    "    %\\begin{enumerate}[(a)]\n",
    "    %  \\item Given a topological sort $V = (v_{i_1},\\ldots,v_{i_{|V|}})$,\n",
    "    %  sequentially compute the layers' outputs, also denoted by $v_{i_j}$.\n",
    "    %  \\item Then $y(x_n;w) = v_{i_{|V|}}$ is the network's output.\n",
    "    %\\end{enumerate}\n",
    "    \\item Compute the loss $\\mathcal{L}_n := \\mathcal{L}(v_{i_{|V|}}, y_n^*)$\n",
    "    and its gradient\n",
    "    \\begin{align}\n",
    "      \\frac{\\partial \\mathcal{L}_n}{\\partial v_{i_{|V|}}}.\n",
    "    \\end{align}\n",
    "    \\vspace{-6px}\n",
    "    \\item For each $j = |V|,\\ldots,1$ compute\n",
    "    \\begin{align}\n",
    "      \\frac{\\partial \\mathcal{L}_n}{\\partial w_j} =\n",
    "      \\frac{\\partial \\mathcal{L}_n}{\\partial v_{i_{|V|}}} \\prod_{k = j + 1}^{|V|} \\frac{\\partial v_{i_k}}{\\partial v_{i_{k - 1}}}\n",
    "      \\frac{\\partial v_{i_j}}{\\partial w_j}.\n",
    "    \\end{align}\n",
    "    where $w_j$ refers to the weights in node $i_j$.\n",
    "    \\vspace{-12px}\n",
    "  \\end{enumerate}\n",
    "  \\caption{Error backpropagation algorithm for a layered neural network\n",
    "  represented as computation graph $G = (V,E)$.}\n",
    "\\end{algorithm}\n",
    "\n",
    "\\end{document}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "00a1ae1e6deaa37076d141dc26cd1e058ef740ecaf1a0d690f5f0ccaa51e4cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
